#!/usr/bin/env python3
"""
Gemini Code CLI - A user-friendly command-line interface for the Gemini-Claude proxy.
"""

import argparse
import os
import sys
import subprocess
import signal
import time
import json
import requests
from pathlib import Path
from typing import Optional, Dict, Any
import shutil

class ClaudeProxyCLI:
    def __init__(self):
        self.script_dir = Path(__file__).parent.absolute()
        
        # Use XDG-compliant config directory structure
        if os.name == 'nt':  # Windows
            self.config_dir = Path.home() / 'AppData' / 'Local' / 'gemini-code'
        else:  # Unix-like (Linux, macOS)
            xdg_config_home = os.environ.get('XDG_CONFIG_HOME')
            if xdg_config_home:
                self.config_dir = Path(xdg_config_home) / 'gemini-code'
            else:
                self.config_dir = Path.home() / '.config' / 'gemini-code'
        
        # Create config directory if it doesn't exist
        self.config_dir.mkdir(parents=True, exist_ok=True)
        
        # Config files in proper locations
        self.config_file = self.config_dir / 'config.env'
        self.pid_file = self.config_dir / 'gemini-code.pid'
        self.default_config = {
            'GEMINI_API_KEY': '',
            'BIG_MODEL': 'gemini-2.0-flash-exp',
            'SMALL_MODEL': 'gemini-1.5-flash-latest',
            'HOST': '0.0.0.0',
            'PORT': '8082',
            'LOG_LEVEL': 'WARNING',
            'MAX_TOKENS_LIMIT': '8192',
            'REQUEST_TIMEOUT': '90',
            'MAX_RETRIES': '2',
            'MAX_STREAMING_RETRIES': '12',
            'FORCE_DISABLE_STREAMING': 'false',
            'EMERGENCY_DISABLE_STREAMING': 'false'
        }

    def load_config(self) -> Dict[str, str]:
        """Load configuration from .env file"""
        config = self.default_config.copy()
        if self.config_file.exists():
            with open(self.config_file, 'r') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        config[key.strip()] = value.strip().strip('"\'')
        return config

    def save_config(self, config: Dict[str, str]):
        """Save configuration to .env file"""
        with open(self.config_file, 'w') as f:
            f.write("# Gemini Code Configuration\n")
            f.write("# Generated by gemini-code CLI\n\n")
            for key, value in config.items():
                f.write(f'{key}="{value}"\n')

    def check_dependencies(self) -> bool:
        """Check if all required dependencies are installed"""
        try:
            import fastapi
            import uvicorn
            import litellm
            import dotenv
            return True
        except ImportError as e:
            print(f"Missing dependency: {e}")
            return False

    def install_dependencies(self):
        """Install required dependencies"""
        print("Installing dependencies...")
        requirements_file = self.script_dir / 'requirements.txt'
        if requirements_file.exists():
            subprocess.run([sys.executable, '-m', 'pip', 'install', '-r', str(requirements_file)], check=True)
        else:
            # Fallback to hardcoded requirements
            deps = [
                'fastapi[standard]>=0.115.11',
                'uvicorn>=0.34.0',
                'httpx>=0.25.0',
                'pydantic>=2.0.0',
                'litellm>=1.40.14',
                'python-dotenv>=1.0.0'
            ]
            subprocess.run([sys.executable, '-m', 'pip', 'install'] + deps, check=True)
        print("Dependencies installed successfully!")

    def is_server_running(self) -> bool:
        """Check if the server is currently running"""
        if not self.pid_file.exists():
            return False
        
        try:
            with open(self.pid_file, 'r') as f:
                pid = int(f.read().strip())
            
            # Check if process is still running
            os.kill(pid, 0)
            return True
        except (OSError, ValueError, ProcessLookupError):
            # Process not found or PID file corrupted
            if self.pid_file.exists():
                self.pid_file.unlink()
            return False

    def get_server_status(self) -> Dict[str, Any]:
        """Get detailed server status"""
        if not self.is_server_running():
            return {'running': False, 'pid': None, 'health': None}
        
        config = self.load_config()
        port = config.get('PORT', '8082')
        
        try:
            with open(self.pid_file, 'r') as f:
                pid = int(f.read().strip())
        except:
            pid = None
        
        # Check health endpoint
        try:
            response = requests.get(f'http://localhost:{port}/health', timeout=5)
            health = response.json() if response.status_code == 200 else None
        except:
            health = None
        
        return {'running': True, 'pid': pid, 'health': health}

    def start_server(self, daemon: bool = True):
        """Start the Gemini proxy server"""
        if self.is_server_running():
            print("Server is already running!")
            return
        
        if not self.check_dependencies():
            print("Installing missing dependencies...")
            self.install_dependencies()
        
        config = self.load_config()
        if not config.get('GEMINI_API_KEY'):
            print("Error: GEMINI_API_KEY not configured. Run 'gemini-code config' first.")
            return
        
        server_py = self.script_dir / 'server.py'
        if not server_py.exists():
            print(f"Error: server.py not found in {self.script_dir}")
            return
        
        # Set environment variables
        env = os.environ.copy()
        env.update(config)
        
        if daemon:
            # Start server as daemon
            print("Starting Gemini proxy server...")
            proc = subprocess.Popen(
                [sys.executable, str(server_py)],
                env=env,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                start_new_session=True
            )
            
            # Save PID
            with open(self.pid_file, 'w') as f:
                f.write(str(proc.pid))
            
            # Wait a moment and check if it started successfully
            time.sleep(2)
            if self.is_server_running():
                port = config.get('PORT', '8082')
                print(f"Server started successfully on port {port}")
                print(f"PID: {proc.pid}")
                print(f"Set ANTHROPIC_BASE_URL=http://localhost:{port} to use with Claude Code")
            else:
                print("Failed to start server. Check logs for errors.")
        else:
            # Start server in foreground
            print("Starting Gemini proxy server in foreground...")
            print("Press Ctrl+C to stop")
            subprocess.run([sys.executable, str(server_py)], env=env)

    def stop_server(self):
        """Stop the Gemini proxy server"""
        if not self.is_server_running():
            print("Server is not running.")
            return
        
        try:
            with open(self.pid_file, 'r') as f:
                pid = int(f.read().strip())
            
            print(f"Stopping server (PID: {pid})...")
            os.kill(pid, signal.SIGTERM)
            
            # Wait for graceful shutdown
            for _ in range(10):
                try:
                    os.kill(pid, 0)
                    time.sleep(0.5)
                except ProcessLookupError:
                    break
            else:
                # Force kill if still running
                print("Forcing server shutdown...")
                os.kill(pid, signal.SIGKILL)
            
            self.pid_file.unlink()
            print("Server stopped successfully.")
            
        except Exception as e:
            print(f"Error stopping server: {e}")
            if self.pid_file.exists():
                self.pid_file.unlink()

    def show_status(self):
        """Show server status"""
        config = self.load_config()
        status = self.get_server_status()
        
        print("Claude Code Proxy Status")
        print("=" * 30)
        
        if status['running']:
            print("‚úÖ Server is running")
            print(f"   PID: {status['pid']}")
            print(f"   Port: {config.get('PORT', '8082')}")
            
            if status['health']:
                health = status['health']
                print(f"   Health: {health.get('status', 'unknown')}")
                print(f"   Version: {health.get('version', 'unknown')}")
                print(f"   API Key: {'configured' if health.get('gemini_api_configured') else 'missing'}")
                
                streaming_config = health.get('streaming_config', {})
                streaming_status = "enabled"
                if streaming_config.get('force_disabled'):
                    streaming_status = "force disabled"
                elif streaming_config.get('emergency_disabled'):
                    streaming_status = "emergency disabled"
                print(f"   Streaming: {streaming_status}")
                
                # Show request statistics if available
                stats = health.get('statistics', {})
                if stats.get('total_requests', 0) > 0:
                    print(f"   Requests: {stats.get('total_requests', 0)} total, {stats.get('successful_requests', 0)} successful")
                    print(f"   Rate: {stats.get('requests_per_minute', 0):.1f} req/min, uptime: {stats.get('uptime_seconds', 0):.0f}s")
            else:
                print("   Health: Unable to connect")
        else:
            print("‚ùå Server is not running")
            print(f"   Port: {config.get('PORT', '8082')} (configured)")
        
        print("\nConfiguration:")
        print(f"   Config Dir: {self.config_dir}")
        print(f"   Big Model: {config.get('BIG_MODEL', 'not set')}")
        print(f"   Small Model: {config.get('SMALL_MODEL', 'not set')}")
        print(f"   Log Level: {config.get('LOG_LEVEL', 'not set')}")
        
        api_key = config.get('GEMINI_API_KEY', '')
        if api_key:
            masked_key = f"{api_key[:8]}...{api_key[-4:]}" if len(api_key) > 12 else "configured"
            print(f"   API Key: {masked_key}")
        else:
            print("   API Key: ‚ùå not configured")
        
        print("\nConnection Info:")
        port = config.get('PORT', '8082')
        print(f"   Local URL: http://localhost:{port}")
        print(f"   Claude Code: ANTHROPIC_BASE_URL=http://localhost:{port}")
        print(f"   Curl Test: curl http://localhost:{port}/health")
        
        if not status['running'] and config.get('GEMINI_API_KEY'):
            print(f"\nüí° To start: claude-proxy start")
        elif not config.get('GEMINI_API_KEY'):
            print(f"\nüí° To configure: claude-proxy config")

    def configure(self):
        """Interactive configuration setup"""
        print("Claude Code Proxy Configuration")
        print("=" * 30)
        print(f"Config will be stored in: {self.config_dir}")
        print("")
        
        # Migrate old config if it exists in repo directory
        self._migrate_old_config()
        
        config = self.load_config()
        
        # API Key
        current_key = config.get('GEMINI_API_KEY', '')
        masked_key = f"{current_key[:8]}..." if len(current_key) > 8 else current_key
        api_key = input(f"Gemini API Key [{masked_key}]: ").strip()
        if api_key:
            config['GEMINI_API_KEY'] = api_key
        
        # Models
        big_model = input(f"Big Model (sonnet/opus) [{config['BIG_MODEL']}]: ").strip()
        if big_model:
            config['BIG_MODEL'] = big_model
        
        small_model = input(f"Small Model (haiku) [{config['SMALL_MODEL']}]: ").strip()
        if small_model:
            config['SMALL_MODEL'] = small_model
        
        # Server settings
        port = input(f"Server Port [{config['PORT']}]: ").strip()
        if port:
            config['PORT'] = port
        
        log_level = input(f"Log Level (DEBUG/INFO/WARNING/ERROR) [{config['LOG_LEVEL']}]: ").strip().upper()
        if log_level in ['DEBUG', 'INFO', 'WARNING', 'ERROR']:
            config['LOG_LEVEL'] = log_level
        
        self.save_config(config)
        print(f"\n‚úÖ Configuration saved to: {self.config_file}")
        
        # Test API key if provided
        if config.get('GEMINI_API_KEY'):
            print("\nTesting API key...")
            if self.test_api_key(config['GEMINI_API_KEY']):
                print("‚úÖ API key is valid!")
                
                # Offer model validation
                validate_choice = input("\nWould you like to validate available models and update config? (y/n): ").lower().strip()
                if validate_choice in ['y', 'yes']:
                    validation_results = self.validate_models(config['GEMINI_API_KEY'], fast_mode=True)
                    if validation_results:
                        update_choice = input("\nUpdate configuration with best available models? (y/n): ").lower().strip()
                        if update_choice in ['y', 'yes']:
                            self.update_config_with_best_models(validation_results)
            else:
                print("‚ùå API key test failed. Please check your key.")
        
        print(f"\nüìÅ Your configuration is safely stored outside the repository at:")
        print(f"   {self.config_dir}")
        print("   This keeps your API keys secure and works from any location.")

    def test_api_key(self, api_key: str) -> bool:
        """Test if the API key is valid"""
        try:
            import litellm
            litellm.api_key = api_key
            response = litellm.completion(
                model="gemini/gemini-1.5-flash-latest",
                messages=[{"role": "user", "content": "test"}],
                max_tokens=5
            )
            return True
        except Exception as e:
            print(f"API test error: {e}")
            return False
    
    def discover_available_models(self, api_key: str) -> list:
        """Discover available models from Gemini API"""
        try:
            import requests
            import json
            
            print("üîç Discovering available models from Gemini API...")
            url = f"https://generativelanguage.googleapis.com/v1beta/models?key={api_key}"
            
            response = requests.get(url, timeout=10)
            if response.status_code == 200:
                data = response.json()
                models = []
                
                for model in data.get('models', []):
                    name = model.get('name', '')
                    if name.startswith('models/'):
                        # Remove 'models/' prefix
                        clean_name = name[7:]
                        display_name = model.get('displayName', clean_name)
                        description = model.get('description', '')
                        
                        # Only include generateContent capable models
                        supported_methods = model.get('supportedGenerationMethods', [])
                        if 'generateContent' in supported_methods:
                            models.append((clean_name, display_name, description))
                
                print(f"‚úÖ Discovered {len(models)} available models")
                return models
            else:
                print(f"‚ö†Ô∏è Model discovery failed (HTTP {response.status_code}), using fallback list")
                return []
                
        except Exception as e:
            print(f"‚ö†Ô∏è Model discovery error: {e}, using fallback list")
            return []
    
    def validate_models(self, api_key: str = None, fast_mode: bool = True) -> dict:
        """Validate which Gemini models are available with the API key"""
        if not api_key:
            config = self.load_config()
            api_key = config.get('GEMINI_API_KEY')
            
        if not api_key:
            print("‚ùå No API key provided")
            return {}
        
        if fast_mode:
            # Fast mode: only test the models we actually care about
            print("üöÄ Fast validation mode - testing key models only...")
            test_models = [
                # Newest and best models first
                ("gemini-2.5-flash", "Gemini 2.5 Flash (Latest)"),
                ("gemini-2.5-pro", "Gemini 2.5 Pro (Most Capable)"),
                ("gemini-2.0-flash-exp", "Gemini 2.0 Flash Experimental"),
                ("gemini-2.0-flash", "Gemini 2.0 Flash"),
                ("gemini-1.5-flash-latest", "Gemini 1.5 Flash (Reliable)"),
                ("gemini-1.5-pro-latest", "Gemini 1.5 Pro (Fallback)"),
            ]
        else:
            # Full discovery mode (for advanced users)
            print("üîç Full discovery mode - this may take a while...")
            discovered_models = self.discover_available_models(api_key)
            
            if discovered_models:
                # Use discovered models, prioritizing by name patterns
                test_models = []
                priority_patterns = [
                    ('2.5-flash', 'Gemini 2.5 Flash'),
                    ('2.5-pro', 'Gemini 2.5 Pro'), 
                    ('2.0-flash', 'Gemini 2.0 Flash'),
                    ('1.5-flash', 'Gemini 1.5 Flash'),
                    ('1.5-pro', 'Gemini 1.5 Pro'),
                ]
                
                # Sort by priority patterns, limit to reasonable number
                for pattern, category in priority_patterns:
                    for name, display, desc in discovered_models:
                        if pattern in name.lower() and name not in [m[0] for m in test_models]:
                            test_models.append((name, f"{display} ({category})"))
                            if len(test_models) >= 15:  # Reasonable limit
                                break
                    if len(test_models) >= 15:
                        break
            else:
                # Fallback to fast mode models
                test_models = [
                    ("gemini-2.5-flash", "Gemini 2.5 Flash (Latest)"),
                    ("gemini-2.5-pro", "Gemini 2.5 Pro (Most Capable)"),
                    ("gemini-2.0-flash-exp", "Gemini 2.0 Flash Experimental"),
                    ("gemini-1.5-flash-latest", "Gemini 1.5 Flash (Reliable)"),
                ]
        
        print("üîç Validating available Gemini models...")
        print("=" * 50)
        
        results = {}
        working_models = []
        
        try:
            import litellm
            import time
            
            for model_name, description in test_models:
                print(f"Testing {description}...", end=" ", flush=True)
                
                try:
                    time.sleep(1)  # Rate limit protection
                    response = litellm.completion(
                        model=f"gemini/{model_name}",
                        messages=[{"role": "user", "content": "hi"}],
                        max_tokens=5,
                        api_key=api_key
                    )
                    print("‚úÖ Available")
                    results[model_name] = "available"
                    working_models.append((model_name, description))
                    
                except Exception as e:
                    error_str = str(e).lower()
                    if "rate limit" in error_str or "quota" in error_str:
                        print("üö´ Rate limited")
                        results[model_name] = "rate_limited"
                    elif "404" in error_str or "not found" in error_str:
                        print("‚ùå Not available")
                        results[model_name] = "not_available"
                    else:
                        print(f"‚ùì Error: {str(e)[:50]}...")
                        results[model_name] = f"error: {str(e)[:50]}"
                        
        except ImportError:
            print("‚ùå LiteLLM not installed. Run: pip install -r requirements.txt")
            return {}
        
        # Summary and recommendations
        print("\nüìä Model Validation Summary")
        print("=" * 50)
        
        if working_models:
            print("‚úÖ Available models:")
            for model, desc in working_models:
                print(f"   ‚Ä¢ {model} - {desc}")
                
            print(f"\nüí° Recommendations:")
            
            # Find best small model
            small_candidates = [m for m in working_models if "flash" in m[0].lower() and "1.5" in m[0]]
            if small_candidates:
                print(f"   Small/Fast model: {small_candidates[0][0]}")
            
            # Find best big model  
            big_candidates = [m for m in working_models if "2.0" in m[0] or ("flash" in m[0].lower() and "2." in m[0])]
            if not big_candidates:
                big_candidates = [m for m in working_models if "pro" in m[0].lower()]
            if not big_candidates and working_models:
                big_candidates = [working_models[0]]  # Fallback to any working model
                
            if big_candidates:
                print(f"   Big/Powerful model: {big_candidates[0][0]}")
        else:
            print("‚ùå No models are currently available")
            print("   This could be due to:")
            print("   ‚Ä¢ API key issues")
            print("   ‚Ä¢ Rate limiting")
            print("   ‚Ä¢ Regional restrictions")
            
        return results
    
    def update_config_with_best_models(self, validation_results: dict):
        """Update configuration with the best available models"""
        config = self.load_config()
        
        # Find working models
        working = [k for k, v in validation_results.items() if v == "available"]
        
        if not working:
            print("‚ö†Ô∏è No working models found, keeping current configuration")
            return False
            
        # Find best small model (prefer newer flash models)
        small_priorities = [
            lambda m: "2.5" in m and "flash" in m,  # 2.5 Flash (newest)
            lambda m: "2.0" in m and "flash" in m,  # 2.0 Flash 
            lambda m: "1.5" in m and "flash" in m,  # 1.5 Flash (reliable)
            lambda m: "flash" in m,                 # Any Flash
        ]
        
        new_small = None
        for priority_check in small_priorities:
            candidates = [m for m in working if priority_check(m)]
            if candidates:
                new_small = candidates[0]
                break
                
        if new_small and new_small != config.get('SMALL_MODEL'):
            print(f"üìù Updating SMALL_MODEL: {config.get('SMALL_MODEL')} ‚Üí {new_small}")
            config['SMALL_MODEL'] = new_small
        
        # Find best big model (prefer newest capable models)
        big_priorities = [
            lambda m: "2.5" in m and "pro" in m,    # 2.5 Pro (most capable)
            lambda m: "2.5" in m and "flash" in m,  # 2.5 Flash (fast + new)
            lambda m: "2.0" in m and "flash" in m,  # 2.0 Flash (good balance)
            lambda m: "1.5" in m and "pro" in m,    # 1.5 Pro (fallback)
            lambda m: "pro" in m,                   # Any Pro
            lambda m: "flash" in m,                 # Any Flash
        ]
        
        new_big = None
        for priority_check in big_priorities:
            candidates = [m for m in working if priority_check(m)]
            if candidates:
                new_big = candidates[0]
                break
        
        if not new_big and working:
            new_big = working[0]  # Absolute fallback
                
        if new_big and new_big != config.get('BIG_MODEL'):
            print(f"üìù Updating BIG_MODEL: {config.get('BIG_MODEL')} ‚Üí {new_big}")
            config['BIG_MODEL'] = new_big
        
        self.save_config(config)
        print("‚úÖ Configuration updated with best available models")
        return True

    def _migrate_old_config(self):
        """Migrate configuration from old repo-based location"""
        old_config_file = self.script_dir / '.env'
        
        if old_config_file.exists() and not self.config_file.exists():
            print("üîÑ Migrating configuration from repository to secure location...")
            try:
                shutil.copy2(old_config_file, self.config_file)
                print(f"‚úÖ Configuration migrated to: {self.config_file}")
                
                # Ask if user wants to remove old config
                response = input("Remove old config file from repository? (y/n): ").lower().strip()
                if response in ['y', 'yes']:
                    old_config_file.unlink()
                    print("‚úÖ Old config file removed from repository")
                else:
                    print("‚ö†Ô∏è  Consider removing the old config file manually for security")
                    
            except Exception as e:
                print(f"‚ùå Migration failed: {e}")
                print("Please manually copy your configuration")

    def show_logs(self, follow: bool = False):
        """Show server logs"""
        # Check multiple possible log locations
        log_locations = [
            self.config_dir / 'logs' / 'gemini-code.log',
            self.script_dir / 'logs' / 'gemini-code.log',
            self.script_dir / 'gemini-code.log'
        ]
        
        log_file = None
        for location in log_locations:
            if location.exists():
                log_file = location
                break
        
        if not log_file:
            print("No log file found. Available log locations:")
            for location in log_locations:
                print(f"  - {location}")
            return
        
        if follow:
            subprocess.run(['tail', '-f', str(log_file)])
        else:
            subprocess.run(['tail', '-50', str(log_file)])

def main():
    parser = argparse.ArgumentParser(
        description='Claude Code Proxy - Multi-provider proxy for Claude Code',
        epilog='Examples:\n'
               '  claude-proxy start                    # Start as daemon\n'
               '  claude-proxy start --foreground       # Start in foreground\n'
               '  LOG_LEVEL=INFO claude-proxy start -f  # Start with request debugging\n'
               '  claude-proxy status                    # Show status and statistics',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    
    # Start command
    start_parser = subparsers.add_parser('start', help='Start the proxy server')
    start_parser.add_argument('--foreground', '-f', action='store_true', 
                             help='Run in foreground with logs visible. Use LOG_LEVEL=INFO for request debugging')
    
    # Stop command
    subparsers.add_parser('stop', help='Stop the proxy server')
    
    # Status command
    subparsers.add_parser('status', help='Show server status')
    
    # Config command
    subparsers.add_parser('config', help='Configure the proxy server')
    
    # Install command
    subparsers.add_parser('install', help='Install dependencies')
    
    # Logs command
    logs_parser = subparsers.add_parser('logs', help='Show server logs')
    logs_parser.add_argument('--follow', '-f', action='store_true', help='Follow log output')
    
    # Models command
    models_parser = subparsers.add_parser('models', help='Validate and manage available models')
    models_parser.add_argument('--update-config', action='store_true', help='Update config with best models')
    models_parser.add_argument('--full-discovery', action='store_true', help='Discover all available models (slow)')
    models_parser.add_argument('--fast', action='store_true', help='Fast mode - test only key models (default)', default=True)
    
    args = parser.parse_args()
    
    cli = ClaudeProxyCLI()
    
    if args.command == 'start':
        cli.start_server(daemon=not args.foreground)
    elif args.command == 'stop':
        cli.stop_server()
    elif args.command == 'status':
        cli.show_status()
    elif args.command == 'config':
        cli.configure()
    elif args.command == 'install':
        cli.install_dependencies()
    elif args.command == 'logs':
        cli.show_logs(follow=args.follow)
    elif args.command == 'models':
        fast_mode = not args.full_discovery  # If full_discovery is True, fast_mode is False
        validation_results = cli.validate_models(fast_mode=fast_mode)
        if args.update_config and validation_results:
            cli.update_config_with_best_models(validation_results)
    else:
        parser.print_help()

if __name__ == '__main__':
    main()